+++
Categories = ["インフラ", "Linux", "HAProxy", "VPS", "vagrant", "ロードバランサ"]
Description = " 前回の【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築に続き、同様の環境を用いて、ロードバランサ構築を行った。 ロードバランサの構築にはHAProxyを利用した。       1. 環境      前回同様で"
Tags = ["tech"]
date = "2014-11-09T17:14:00+09:00"
title = "【VPS1台でインフラ勉強】HAProxyでロードバランサーを構築"
author = "mosuke5"
archive = ["2014"]
draft = false
+++

こんにちは。もーすけです。  
前回の[【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築](https://blog.mosuke.tech/entry/2014/10/09/230555)に続き、同様の環境を用いて、ロードバランサ構築を行っていきます。
ロードバランサの構築には[HAProxy](entry/2014/11/09/171436/)を利用します。

- 第1回 [サーバ複数台構成、Nginxでリバースプロキシ構築](https://blog.mosuke.tech/entry/2014/10/09/230555)
- 第2回 HAProxyでロードバランサ構築

## 1. 環境
前回同様で、さくらVPSの1GBのプラン1台のみ利用します。もちろん、ローカルのPCでやっていただいても構いません。VPSでわざわざやっている理由は、人と共同でやっていたため、共有環境におきたかったのと、これからさくらのVPS上で作っていく予定があるからです。

- メモリ：１GB
- CPU：仮想２コア
- HDD：100GB
- OS：CentOS7
- サーバ仮想化：<a class="keyword" href="http://d.hatena.ne.jp/keyword/Vagrant">Vagrant</a>(Utuntu13)
- ロードバランサ：<a href="http://www.haproxy.org/">HAProxy - The Reliable, High Performance TCP/HTTP Load Balancer</a>

## 2. 構成図
非常に簡単ですが、下記の構成を実際に作っていきます。  
<span itemscope itemtype="http://schema.org/Photograph"><img src="https://cdn-ak.f.st-hatena.com/images/fotolife/m/mosuke5/20141109/20141109170337.png" alt="f:id:mosuke5:20141109170337p:plain" title="f:id:mosuke5:20141109170337p:plain" class="hatena-fotolife" itemprop="image"></span>

## 3. ロードバランサの構築
### ■ホストサーバ側の設定
インストールはyumで入れることが可能です。
```
#HAProxyインストール
$ sudo yum install haproxy
```

```
#設定はすごく簡単で以下のファイルのみ。実際に
$ sudo vim /etc/haproxy/haproxy.cfg

#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global

    log         127.0.0.1 local6 debug

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults

    ##ロードバランサの動作モード。tcpにするとL4ロードバランサになる。httpにするとL7ロードバランサ。
    mode                    http
    log                     global
    option              log-health-checks
    option                  httplog
    option                  dontlognull
    
    ##ヘルスチェック用のhtmlファイルをWebサーバ側に設置している。設置については後述。
    option httpchk GET /health_check.html HTTP/1.0\r\nUser-agent:\ Proxy-Check

    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
frontend  main *:80
    default_backend             hoge

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend hoge
    balance     roundrobin
    server      web10 192.168.33.10:80 check inter 3000 fall 2 rise 2
    server      web11 192.168.33.11:80 check inter 3000 fall 2 rise 2 
```

ロードバランサを80番ポートで待ち受けたい場合には、ApacheやNginxが起動していないことは確認しておきましょう。
立ち上がっているとそもそも、HAProxyの起動に失敗するはずです。

```
$ sudo systemctl stop nginx
$ sudo systemctl stop httpd 
```

HAProxyのログをsyslogに残すように設定します。  
ただしメインのmessageではなく、独自のファイルに書くために以下の設定します。

```
##ログの格納場所作成
$ sudo mkdir /var/log/haproxy

##syslogの設定変更
$ vim /etc/rsyslog.d/haproxy.conf
   $ModLoad imudp
   $UDPServerRun 514
   $template Haproxy,"%msg%\n"
   local6.* -/var/log/haproxy/haproxy.log;Haproxy

$ vim /etc/sysconfig/rsyslog
以下を追加
SYSLOGD_OPTIONS="-r"

## haproxyの起動
$ sudo systemctl start haproxy 
```

### L4, L7のロードバランサ
次に行く前にL4とL7ロードバランサの違いについて確認します。
こちらは非常に重要な概念なので、HAProxyをはじめ、AWSなどのクラウドでELBロードバランサを利用する際も役に立ちますので、かならず抑えておきましょう。

L4ロードバランサは、名前の通りL4でのロードバランサになりますので、実際のHTTPの処理はバックエンドのサーバで行います。
一方、L7ロードバランサはどちらかというとリバースプロキシ的な役割で、ロードバランサ自身がHTTPの処理を行います。
その後にバックエンドのサーバに対してもう一度HTTPでリクエストを行います。

どう使い分けるべきか？  
それはどの処理をどのサーバで行わせたいか？に基本つきます。
たとえば、L7ロードバランサを使用した場合は、SSLの終端をロードバランサで行うことが可能です。
実際のユーザから見えるIPアドレスのホストとの間はロードバランサになるなりますので。
バックエンドのサーバとSSL通信をするかは、もっと内部的な要件によって決められるということになります。

WebサーバでもあえてL4のロードバランサを使いたいこともあります。  
この記事を書いた数年後の話になるのですが、HTTP2に対応したサイトを作りたいと考えましたが、ロードバランサが対応していなかったため、ロードバランサはL4で受け付けて、L7をバックエンドのサーバで受け付けるようにしました。バックエンドのサーバでは最新版のNginxに変更が可能で新しいプロトコルへの対応も可能だったためです。

下記サイトも参考にしてみてください。  
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://www.atmarkit.co.jp/ait/articles/0302/05/news001.html" data-iframely-url="//cdn.iframe.ly/GegycpB?iframe=card-small"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>

### ■Webサーバの設定
バックエンドのWebサーバ構築は省くが、Apache2をインストールしただけです。  
仮想でのサーバ構築は前回を参照。
<a href="https://blog.mosuke.tech/entry/2014/10/09/230555">【VPS1台でインフラ勉強】サーバ複数台構成、Nginxでリバースプロキシ構築</a>


ヘルスチェック用のhtml設置しておくとわかりやすいです。  
負荷分散されていることを確認するために、内容の異なるヘルスチェック用のHTMLをおいておくと非常にわかりやすいです。
```
## /var/www/html配下にヘルスチェック用のhtml設置
$ sudo touch health_check.html 
```

## 4. 動作試験
ブラウザよりホストサーバへアクセスします。
きちんとロードバランスされていることを確認します。確認方法は、もしヘルスチェック用のHTMLの中身を変えていればそれでわかるとおもいます。あるいはログから確認する必要があります。  
HAProxy側のログは以下のとおり。

```
$ sudo tail -f /var/log/haproxy/haproxy.conf

##起動した時。L7のhealt checkが走っている
Proxy main started.
Proxy hoge started.
Health check for server hoge/web10 succeeded, reason: Layer7 check passed, code: 200, info: "OK", check duration: 33ms, status: 2/2 UP.
Health check for server hoge/web11 succeeded, reason: Layer7 check passed, code: 200, info: "OK", check duration: 12ms, status: 2/2 UP.

##webサーバ側でapacheを停止
Health check for server hoge/web11 failed, reason: Layer7 wrong status, code: 404, info: "Not Found", check duration: 13ms, status: 1/2 UP.
Health check for server hoge/web11 failed, reason: Layer7 wrong status, code: 404, info: "Not Found", check duration: 8ms, status: 0/2 DOWN.

##webサーバ側でhealth_check.htmlを削除した時も同様に
Health check for server hoge/web11 failed, reason: Layer7 wrong status, code: 404, info: "Not Found", check duration: 13ms, status: 1/2 UP.
Health check for server hoge/web11 failed, reason: Layer7 wrong status, code: 404, info: "Not Found", check duration: 8ms, status: 0/2 DOWN.

##webサーバ側でhealth_check.htmlを復活させた時
Health check for server hoge/web11 succeeded, reason: Layer7 check passed, code: 200, info: "OK", check duration: 8ms, status: 1/2 DOWN.
Health check for server hoge/web11 succeeded, reason: Layer7 check passed, code: 200, info: "OK", check duration: 6ms, status: 2/2 UP.
Server hoge/web11 is UP. 2 active and 0 backup servers online. 0 sessions requeued, 0 total in queue. 
```

Webサーバ側の<a class="keyword" href="http://d.hatena.ne.jp/keyword/Apache">Apache</a>ログを見てみると。

```
$ sudo tail -f /var/log/apache2/access.log

##ロードバランサからのヘルスチェックが来ていることがわかる。
192.168.33.1 - - [09/Nov/2014:08:07:43 +0000] "GET /health_check.html HTTP/1.0" 200 276 "-" "Proxy-Check"
192.168.33.1 - - [09/Nov/2014:08:07:46 +0000] "GET /health_check.html HTTP/1.0" 200 276 "-" "Proxy-Check"
192.168.33.1 - - [09/Nov/2014:08:07:49 +0000] "GET /health_check.html HTTP/1.0" 200 276 "-" "Proxy-Check"
192.168.33.1 - - [09/Nov/2014:08:07:52 +0000] "GET /health_check.html HTTP/1.0" 200 276 "-" "Proxy-Check"

##Webからのアクセスが来た場合
##SorceのIPはロードバランサにIPになっているが、UserAgentなど書き込まれていることを確認。
192.168.33.1 - - [09/Nov/2014:08:10:06 +0000] "GET / HTTP/1.1" 200 488 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25" 
```

## まとめ
HAProxyのインストール設定自体は非常に簡単で、ロードバランサといえど恐れる必要はなかったです。  
それ以上にやはり重要なのは、L4/L7ロードバランサの違いやその使いみち。ここを理解しておくことが非常に重要です。
クラウド時代になり、ロードバランサを用意する敷居が今まで以上に低くなってきたからこそ、そこの概念を抑えることは必須です。